{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************bilibili小黑屋爬虫************\n",
      "****************************************\n",
      "输入main爬取主界面信息，输入opinion爬取评论界面信息opinion\n",
      "当前爬取的案例1503421\n",
      "当前爬取的案例1503422\n",
      "当前爬取的案例1503423\n",
      "当前爬取的案例1503424\n",
      "当前爬取的案例1503425\n",
      "当前爬取的案例1503426\n",
      "当前爬取的案例1503427\n",
      "当前爬取的案例1503428\n",
      "当前爬取的案例1503429\n",
      "当前爬取的案例1503430\n",
      "当前爬取的案例1503431\n",
      "当前爬取的案例1503432\n",
      "当前爬取的案例1503433\n",
      "当前爬取的案例1503434\n",
      "当前爬取的案例1503435\n",
      "当前爬取的案例1503436\n",
      "当前爬取的案例1503437\n",
      "当前爬取的案例1503438\n",
      "当前爬取的案例1503439\n",
      "当前爬取的案例1503440\n",
      "当前爬取的案例1503441\n",
      "当前爬取的案例1503442\n",
      "当前爬取的案例1503443\n",
      "当前爬取的案例1503444\n",
      "当前爬取的案例1503445\n",
      "当前爬取的案例1503446\n",
      "当前爬取的案例1503447\n",
      "当前爬取的案例1503448\n",
      "当前爬取的案例1503449\n",
      "当前爬取的案例1503450\n",
      "当前爬取的案例1503451\n",
      "当前爬取的案例1503452\n",
      "当前爬取的案例1503453\n",
      "当前爬取的案例1503454\n",
      "当前爬取的案例1503455\n",
      "当前爬取的案例1503456\n",
      "当前爬取的案例1503457\n",
      "当前爬取的案例1503458\n",
      "当前爬取的案例1503459\n",
      "当前爬取的案例1503460\n",
      "当前爬取的案例1503461\n",
      "当前爬取的案例1503462\n",
      "当前爬取的案例1503463\n",
      "当前爬取的案例1503464\n",
      "当前爬取的案例1503465\n",
      "当前爬取的案例1503466\n",
      "当前爬取的案例1503467\n",
      "当前爬取的案例1503468\n",
      "当前爬取的案例1503469\n",
      "当前爬取的案例1503470\n",
      "当前爬取的案例1503471\n",
      "当前爬取的案例1503472\n",
      "当前爬取的案例1503473\n",
      "当前爬取的案例1503474\n",
      "当前爬取的案例1503475\n",
      "当前爬取的案例1503476\n",
      "当前爬取的案例1503477\n",
      "当前爬取的案例1503478\n",
      "当前爬取的案例1503479\n",
      "当前爬取的案例1503480\n",
      "当前爬取的案例1503481\n",
      "当前爬取的案例1503482\n",
      "当前爬取的案例1503483\n",
      "当前爬取的案例1503484\n",
      "当前爬取的案例1503485\n",
      "当前爬取的案例1503486\n",
      "当前爬取的案例1503487\n",
      "当前爬取的案例1503488\n",
      "当前爬取的案例1503489\n",
      "当前爬取的案例1503490\n",
      "当前爬取的案例1503491\n",
      "当前爬取的案例1503492\n",
      "当前爬取的案例1503493\n",
      "当前爬取的案例1503494\n",
      "当前爬取的案例1503495\n",
      "当前爬取的案例1503496\n",
      "当前爬取的案例1503497\n",
      "当前爬取的案例1503498\n",
      "当前爬取的案例1503499\n",
      "当前爬取的案例1503500\n",
      "当前爬取的案例1503501\n",
      "当前爬取的案例1503502\n",
      "当前爬取的案例1503503\n",
      "当前爬取的案例1503504\n",
      "当前爬取的案例1503505\n",
      "当前爬取的案例1503506\n",
      "当前爬取的案例1503507\n",
      "当前爬取的案例1503508\n",
      "当前爬取的案例1503509\n",
      "当前爬取的案例1503510\n",
      "当前爬取的案例1503511\n",
      "当前爬取的案例1503512\n",
      "当前爬取的案例1503513\n",
      "当前爬取的案例1503514\n",
      "当前爬取的案例1503515\n",
      "当前爬取的案例1503516\n",
      "当前爬取的案例1503517\n",
      "当前爬取的案例1503518\n",
      "当前爬取的案例1503519\n",
      "当前爬取的案例1503520\n",
      "当前爬取的案例1503521\n",
      "当前爬取的案例1503522\n",
      "当前爬取的案例1503523\n",
      "当前爬取的案例1503524\n",
      "当前爬取的案例1503525\n",
      "当前爬取的案例1503526\n",
      "当前爬取的案例1503527\n",
      "当前爬取的案例1503528\n",
      "当前爬取的案例1503529\n",
      "当前爬取的案例1503530\n",
      "当前爬取的案例1503531\n",
      "当前爬取的案例1503532\n",
      "当前爬取的案例1503533\n",
      "当前爬取的案例1503534\n",
      "当前爬取的案例1503535\n",
      "当前爬取的案例1503536\n",
      "当前爬取的案例1503537\n",
      "当前爬取的案例1503538\n",
      "当前爬取的案例1503539\n",
      "当前爬取的案例1503540\n",
      "当前爬取的案例1503541\n",
      "当前爬取的案例1503542\n",
      "当前爬取的案例1503543\n",
      "当前爬取的案例1503544\n",
      "当前爬取的案例1503545\n",
      "当前爬取的案例1503546\n",
      "当前爬取的案例1503547\n",
      "当前爬取的案例1503548\n",
      "当前爬取的案例1503549\n",
      "当前爬取的案例1503550\n",
      "当前爬取的案例1503551\n",
      "当前爬取的案例1503552\n",
      "当前爬取的案例1503553\n",
      "当前爬取的案例1503554\n",
      "当前爬取的案例1503555\n",
      "当前爬取的案例1503556\n",
      "当前爬取的案例1503557\n",
      "当前爬取的案例1503558\n",
      "当前爬取的案例1503559\n",
      "当前爬取的案例1503560\n",
      "当前爬取的案例1503561\n",
      "当前爬取的案例1503562\n",
      "当前爬取的案例1503563\n",
      "当前爬取的案例1503564\n",
      "当前爬取的案例1503565\n",
      "当前爬取的案例1503566\n",
      "当前爬取的案例1503567\n",
      "当前爬取的案例1503568\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from shutil import copy\n",
    "from urllib import request\n",
    "\n",
    "# Headers.\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "# Control max id and cid.\n",
    "id_Max = '1288800'\n",
    "cid_Max = ['1495','38032','74569','111106','147643','184180','220717','257254','293791','330328',\n",
    "           '366865','403402','439939','476476','513013','549550','586087','622624','659161','695698',\n",
    "           '732235','768772','805309','841846','878383','914920','951457','987994','1024531','1061068',\n",
    "           '1097605','1134142','1170679','1207216','1243753','1280290','1316827','1353364','1389901','1426438',\n",
    "           '1462975','1499512','1535640']\n",
    "cid_Min = ['1068','37605','74142','110679','147216','183753','220290','256827','293364','329901',\n",
    "           '366438','402975','439512','476049','512586','549123','585660','622197','658734','695271',\n",
    "           '731808','768345','804882','841419','877956','914493','951030','987567','1024104','1060641',\n",
    "           '1097178','1133715','1170252','1206789','1243326','1279863','1316400','1352937','1389474','1426011',\n",
    "           '1462548','1499085','1535622']\n",
    "# Save in csv files.\n",
    "def save_csv(filename, info):\n",
    "    with open(filename, 'a', encoding='utf-8', newline=\"\") as f:\n",
    "        if 'main' in filename: fieldnames = ['id', 'cid', 'originContentModify', 'punishType', 'punishTypeName', 'reasonType', 'reasonTypeName']\n",
    "        if 'opinion' in filename: fieldnames = ['opid', 'vote', 'content', 'cid']\n",
    "        writer = csv.DictWriter(f, fieldnames = fieldnames)\n",
    "        writer.writerow(info)\n",
    "\n",
    "# Connect html page and get text.\n",
    "def get_html_text(judge):\n",
    "    # Scanned id and case id.\n",
    "    cid = 1503420#int(cid_Max[41]) - 1 #min cid each time download needed to edit\n",
    "    id = 0\n",
    "    cidmax = int(cid_Min[42]) #max cid each time download needed to edit\n",
    "    # Api for main and opinion blackroom pages.\n",
    "    if judge == 'main':\n",
    "        while id < int(id_Max):\n",
    "            id += 1\n",
    "            url_main = 'https://api.bilibili.com/x/credit/blocked/info?jsonp=jsonp&id={}'.format(id)\n",
    "            html = requests.get(url_main, headers = headers)\n",
    "            dicts = json.loads(html.content)\n",
    "            if (dicts['code'] == -404):\n",
    "                print('该id没有内容'.format(id))\n",
    "                continue\n",
    "            get_main_info(dicts['data'])\n",
    "            print('当前爬取的id{}'.format(id))\n",
    "\n",
    "    if judge == 'opinion':\n",
    "        while cid < cidmax:\n",
    "            cid += 1\n",
    "            page_index = 0\n",
    "            # flag = False # judge None\n",
    "            while True:\n",
    "                page_index += 1\n",
    "                url_opinion = 'https://api.bilibili.com/x/credit/jury/case/opinion?jsonp=jsonp&cid={}&pn={}&ps=10'.format(\n",
    "                    cid,\n",
    "                    page_index)\n",
    "                html = requests.get(url_opinion, headers = headers)\n",
    "                dicts = json.loads(html.content)\n",
    "                if (dicts['data']['opinion'] == None or page_index > 1):\n",
    "                    break\n",
    "                get_opinion_info(cid, dicts['data']['opinion'])\n",
    "            print('当前爬取的案例{}'.format(cid))\n",
    "\n",
    "# Process main page data, extract originContentModify, punishType, punishTypeName, reasonType, reasonTypeName, caseId\n",
    "def get_main_info(data):\n",
    "    for i in data:\n",
    "        Tmp = i['originContentModify']\n",
    "        Tmp = ((str(Tmp).replace(\"'\", '\"')).replace('True', 'true')).replace('False', 'false')\n",
    "        re_h = re.compile('</?\\w+[^>]*>')\n",
    "        Tmp = re_h.sub('', Tmp)\n",
    "        originContentModify, sep, tail = Tmp.partition('批注')\n",
    "        #originContentModify = Tmp.replace('(?<批注).*','')\n",
    "        punishType = i['punishType']\n",
    "        punishTypeName = i['punishTypeName']\n",
    "        reasonType = i['reasonType']\n",
    "        reasonTypeName = i['reasonTypeName']\n",
    "        caseId = i['caseId']\n",
    "        id = i['id']\n",
    "        info = {\n",
    "            'id': id,\n",
    "            'cid': caseId,\n",
    "            'originContentModify': originContentModify,\n",
    "            'punishType': punishType,\n",
    "            'punishTypeName': punishTypeName,\n",
    "            'reasonType': reasonType,\n",
    "            'reasonTypeName': reasonTypeName\n",
    "        }\n",
    "        filename = 'main.csv'\n",
    "        save_csv(filename, info)\n",
    "\n",
    "# Process opinion page opinion, extract vote, content, opid\n",
    "def get_opinion_info(cid, opinion):\n",
    "    for i in opinion:\n",
    "        vote = i['vote']\n",
    "        content = i['content'].replace('\\n', '').replace('\\r', '').replace( \"\\'\", \"\" ).replace( \"\\\"\", \"\" )\n",
    "        opid = i['opid']\n",
    "        info = {\n",
    "            'cid': cid,\n",
    "            'opid': opid,\n",
    "            'vote': vote,\n",
    "            'content': content\n",
    "        }\n",
    "        filename = 'opinion.csv'\n",
    "        save_csv(filename, info)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filename = 'main.csv'\n",
    "    with open(filename, 'w', encoding='utf-8', newline=\"\") as f:\n",
    "        fieldnames = ['id', 'cid', 'originContentModify', 'punishType', 'punishTypeName', 'reasonType',\n",
    "                                             'reasonTypeName']\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "    filename = 'opinion.csv'\n",
    "    with open(filename, 'w', encoding='utf-8', newline=\"\") as f:\n",
    "        fieldnames = ['opid', 'vote', 'content', 'cid']\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "    print('************bilibili小黑屋爬虫************')\n",
    "    print('****************************************')\n",
    "    judge = input('输入main爬取主界面信息，输入opinion爬取评论界面信息')\n",
    "    while judge != 'main' and judge != 'opinion':\n",
    "        judge = input('您的输入有误，请输入main爬取主界面信息，输入opinion爬取评论界面信息，输入0结束程序')\n",
    "        if judge == '0':\n",
    "            os._exit()\n",
    "    get_html_text(judge)\n",
    "    save_path = 'E:\\\\USB\\\\Spider\\\\' + cid_Max[41] + '-' + cid_Min[42] + '.csv' #save path each time download needed to edit\n",
    "    copy('C:\\\\Users\\\\Vivian Meng\\\\Documents\\\\Graduate\\\\opinion.csv', save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "、3\n",
      "39\n",
      "10\n",
      "54\n",
      "2018\n",
      "1900\n",
      "283\n",
      "/22817678）\n",
      "4:02\n",
      "8000\n",
      "（5\n",
      "2:30\n",
      "3》\n",
      "17\n",
      "2333333333333\n",
      "666666666666666\n",
      "02\n",
      "0、\n",
      "6）。\n",
      "8615924、\n",
      "01：23\n",
      "21417022\n",
      "_5215587\n",
      "87103743（\n",
      "8⃣️\n",
      "423\n",
      "55\n",
      "2104\n",
      "。<(￣3￣)>\n",
      "13:07\n",
      "002001\n",
      "2007\n",
      "09\n",
      "33\n",
      "2017\n",
      "2057\n",
      "）2。\n",
      "640*320\n",
      "2012\n",
      "1/4\n",
      "8618588、\n",
      "125\n",
      "90%\n",
      "（1/500）\n",
      "。8\n",
      "79\n",
      "341\n",
      "48\n",
      "#204\n",
      "233333333\n",
      "1066\n",
      "8617761、\n",
      "？1:\n",
      "5~7\n",
      "2:3\n",
      "：1\n",
      "。11\n",
      "300\n",
      "8614182、\n",
      "231\n",
      "1140474\n",
      "（2333）\n",
      "。190191193\n",
      "？233\n",
      "95+\n",
      "1358\n",
      "7000\n",
      "50%~70%\n",
      "416\n",
      "~233\n",
      "310\n",
      "；2\n",
      "2194\n",
      "14***\n",
      "913\n",
      "2333。\n",
      "(:3_ヽ)_\n",
      "6。\n",
      "478\n",
      "┗┃・■・┃┛2333\n",
      "76\n",
      "5、6\n",
      "1508944807（\n",
      "5670\n",
      "1998\n",
      "1～3\n",
      "43\n",
      "《98\n",
      "_(:з」∠)_2333\n",
      "3？\n",
      "13***\n",
      "？？106\n",
      "1:27\n",
      "275\n",
      "08\n",
      "69%\n",
      "1965\n",
      "8090\n",
      "290%。\n",
      "40%\n",
      "65\n",
      "300+\n",
      "6666\n",
      "**29675630\n",
      "（281282\n",
      "。10\n",
      "200+\n",
      "144\n",
      "、2\n",
      "（12\n",
      "24\n",
      "431\n",
      "（985）\n",
      "1968\n",
      "2001\n",
      "62966298\n",
      "123\n",
      "？3\n",
      "328\n",
      "35\n",
      "820\n",
      "=158\n",
      "。3\n",
      "20171126\n",
      "1920\n",
      "99%\n",
      "2）\n",
      "512\n",
      "）3\n",
      "。1、\n",
      "148\n",
      "2019\n",
      "80\n",
      "57\n",
      "28541?\n",
      "。。。1\n",
      "=13\n",
      "（4\n",
      "127\n",
      "？1\n",
      "850\n",
      "。2（\n",
      "32000\n",
      "292\n",
      "3：1\n",
      "2/3\n",
      "4396\n",
      "1060\n",
      "68\n",
      "#2194\n",
      "2009——\n",
      "。2：\n",
      "826\n",
      "8613875、\n",
      "220\n",
      "……233\n",
      "4396’\n",
      "2020\n",
      "？？？4\n",
      "176302**\n",
      "227\n",
      "120\n",
      "258\n",
      "7102\n",
      "30\n",
      "910\n",
      "136639947\n",
      "3300+\n",
      "183\n",
      "==666666666\n",
      "2⃣️\n",
      "（2008\n",
      "56\n",
      "16~18\n",
      "8616305、\n",
      "～5\n",
      "253\n",
      "#2190\n",
      "29\n",
      "99\n",
      "173\n",
      "233333\n",
      "007\n",
      "262\n",
      "122\n",
      "102\n",
      "。4、\n",
      "/201708/08/\n",
      "75\n",
      "05:22、07:44\n",
      "251\n",
      "、255\n",
      "2：1\n",
      "4396。。\n",
      "357815383940424347484961\n",
      "70\n",
      "/26528454/\n",
      "。4\n",
      "2000\n",
      "567\n",
      "46~50\n",
      "36\n",
      "232\n",
      "÷5＝66\n",
      "100%\n",
      "60%\n",
      "500\n",
      "3:15—3:25\n",
      "2008\n",
      "16）\n",
      "2：\n",
      "8、9\n",
      "130\n",
      "3090\n",
      "129\n",
      "92%\n",
      "79114）\n",
      "。。2\n",
      "0405\n",
      "。。17\n",
      "20191111\n",
      "。[1]\n",
      "300%\n",
      "。。1\n",
      "165\n",
      "213\n",
      "＝233\n",
      "272\n",
      "#7\n",
      "1893\n",
      "+3\n",
      "05\n",
      "8616603、\n",
      "2190\n",
      "2*3\n",
      "3408\n",
      "155\n",
      "2077\n",
      "3080\n",
      "1430、1433\n",
      "*2\n",
      "263\n",
      "1:07\n",
      "428\n",
      "1800\n",
      "2013\n",
      "23\n",
      "8。\n",
      "2600\n",
      "1050\n",
      "2018032917:19:31\n",
      "350%\n",
      "13561361\n",
      "。2018\n",
      "1994\n",
      "。(*￣3￣)╭♡\n",
      "10086\n",
      "。7\n",
      "42\n",
      "666\n",
      "342\n",
      "383622**\n",
      "*999）\n",
      "6（\n",
      "80%\n",
      "360\n",
      "2:53\n",
      "=3\n",
      "10%\n",
      "1911：\n",
      "03:48\n",
      "40\n",
      "200\n",
      "61\n",
      "235\n",
      "？5\n",
      "41\n",
      "8614612、\n",
      "。6\n",
      "123456\n",
      "4、\n",
      "03\n",
      "1997\n",
      "~？22\n",
      "。2\n",
      "#15\n",
      "88\n",
      "#110\n",
      "=5305\n",
      "49\n",
      "4/5\n",
      ":1\n",
      "9:30\n",
      "1999\n",
      "20。\n",
      "00\n",
      "(11\n",
      "78777596\n",
      ";4\n",
      "150。（\n",
      "23333333333？\n",
      "50`80\n",
      "；2015\n",
      "4？\n",
      "99999%\n",
      "♂1:♀1、\n",
      "。2010\n",
      "？？97\n",
      "789\n",
      "233333333333333\n",
      "123250\n",
      "1524000\n",
      "#178\n",
      "404\n",
      "52）\n",
      "5000\n",
      "7012）\n",
      "(^0^)/\n",
      "2333)\n",
      "354\n",
      "8～14\n",
      "79~89\n",
      "17、18、19、20、28、31、77、79、81、106、107、108\n",
      "534\n",
      "1080\n",
      "……7\n",
      "_(：3」∠)_\n",
      "2010\n",
      "/41015593\n",
      "687\n",
      "2:\n",
      "8848\n",
      "31\n",
      "（7\n",
      "2014\n",
      "2740\n",
      "；3\n",
      "2/5\n",
      "208\n",
      "15\n",
      "；2、\n",
      "**×2\n",
      "：1、\n",
      "6000\n",
      "3、\n",
      "2017080419:35\n",
      "_(:3」∠)_\n",
      "59\n",
      "5％\n",
      "204\n",
      "1956\n",
      "51\n",
      "999%\n",
      "346\n",
      "66\n",
      "5~10\n",
      "379\n",
      "、+1\n",
      "（*＾3＾）\n",
      "0203\n",
      "198\n",
      "150\n",
      "8070\n",
      "》44\n",
      "2080\n",
      "06\n",
      "2、\n",
      "240\n",
      "520\n",
      "77】\n",
      "：2014\n",
      "46864\n",
      "3700\n",
      "280\n",
      "233\n",
      "45\n",
      "855\n",
      "_(:3」∠❀)_\n",
      "685\n",
      "1539\n",
      "2017416\n",
      "652\n",
      "*1；\n",
      "1206\n",
      "2、7、8\n",
      "17：32\n",
      "2800\n",
      "1+1=2\n",
      "2006\n",
      "？#33\n",
      "26\n",
      "666。\n",
      "？)2\n",
      "1⃣️\n",
      "731\n",
      "8613253\n",
      "50%\n",
      "。19\n",
      "003\n",
      "1/10\n",
      "104\n",
      "48198\n",
      "5:35\n",
      "330\n",
      "1996\n",
      "34\n",
      "4500\n",
      "。。。233333\n",
      "210\n",
      "。4746454441\n",
      "(¦3[▓▓]\n",
      "@78142928446_\n",
      "38\n",
      "8？\n",
      "……（9102\n",
      "2015\n",
      "。3、\n",
      "0:15\n",
      "43999706（\n",
      "#682\n",
      "2050\n",
      "11\n",
      "980\n",
      "89\n",
      "600\n",
      "3(\n",
      "1891\n",
      "？13\n",
      "2333333333\n",
      "=1\n",
      "2333333\n",
      "299\n",
      "。240\n",
      "369\n",
      "72\n",
      "815\n",
      "；#111\n",
      "3~4\n",
      "12447176294\n",
      "192\n",
      ";2\n",
      "？10\n",
      "1350\n",
      "（2\n",
      "140\n",
      "9102\n",
      "96\n",
      "20180924\n",
      "3)\n",
      "810\n",
      "9877％\n",
      "1/2\n",
      "100\n",
      "25\n",
      "37\n",
      "2016\n",
      "))￣0￣)\n",
      "6?\n",
      "334\n",
      "20%\n",
      "151\n",
      "+100\n",
      "2017081719:35\n",
      "4399。\n",
      "！9012\n",
      "60\n",
      "163\n",
      "11912:41:59\n",
      "1540\n",
      "411\n",
      "/19708222\n",
      "666？？\n",
      "40003\n",
      "2、4、5、6、29\n",
      "181\n",
      "?2017081719:30\n",
      "1982\n",
      "1%\n",
      "8102\n",
      "1：\n",
      "+1\n",
      "。(○´3`)\n",
      "600+\n",
      "2/3。\n",
      "×2\n",
      "56255630\n",
      "15+\n",
      "1213\n",
      "#431\n",
      "？4\n",
      "1、\n",
      "？2\n",
      "400\n",
      "285\n",
      "66666\n",
      "87\n",
      "。0\n",
      "2233\n",
      "730\n",
      "19\n",
      "79114\n",
      "1：53\n",
      "70%\n",
      "261\n",
      "931777\n",
      "21\n",
      "？2017103119:25:01\n",
      "502。\n",
      "+999999\n",
      "1400\n",
      "728\n",
      "2017081719:114\n",
      "985\n",
      "7~15\n",
      "！20\n",
      "1+1＝3\n",
      "26#\n",
      "！3\n",
      "648\n",
      "3：15\n",
      "98\n",
      "201\n",
      "6？\n",
      "1957\n",
      "3~5\n",
      "18938380558\n",
      "78\n",
      "18\n",
      "101\n",
      "（#3022#3023#3024#3026#3027#3028#3030#3032#3033#3034）\n",
      "9222901\n",
      "110\n",
      "2。\n",
      "/201708/02/\n",
      "……59\n",
      "123*6\n",
      "267（\n",
      "0202\n",
      "75%\n",
      "14\n",
      "2333\n",
      "12\n",
      "30~40\n",
      "715\n",
      "160\n",
      "23333\n",
      "01\n",
      "12？\n",
      "31144\n",
      "27\n",
      "1282\n",
      "；1（\n",
      "128\n",
      "。1\n",
      "(1233250999)\n",
      "2666\n",
      "77\n",
      "666？\n",
      "555\n",
      "2020222\n",
      "47\n",
      "3000\n",
      "9012\n",
      "？2333\n",
      "9000\n",
      "1000\n",
      "9137\n",
      "10：47\n",
      "12138:\n",
      "。233\n",
      "46\n",
      "#109\n",
      "22\n",
      "1156299\n",
      "132\n",
      "。5\n",
      "50\n",
      "999\n",
      "91\n",
      "13\n",
      "9566\n",
      "4399\n",
      "151515151515\n",
      "/33112481\n",
      "？(:3_ヽ)_\n",
      "20\n",
      "16\n",
      "90\n",
      "5500\n",
      "。2020\n",
      "4、5、6、7\n",
      "6668697172\n",
      "9=\n",
      "。2、\n",
      "69\n",
      "28\n",
      "16》\n",
      "551\n",
      "800\n",
      "00：06\n",
      "705\n",
      "01%\n",
      "152\n",
      "211\n",
      "32\n",
      "2974\n",
      "*4\n",
      "250\n",
      ";3\n",
      "8615395、\n",
      "5561\n",
      "(－1)\n",
      "\n",
      "？(≖_≖)\n",
      "（×\n",
      "（✔）\n",
      "。。。。。。。\n",
      "(￣_\n",
      "(´◑д◐｀)\n",
      "?(\n",
      "(*へ*)\n",
      "ฅ(̳•◡•̳)ฅ）\n",
      "╮(￣⊿￣)╭\n",
      "*：\n",
      "(˶˚ᗨ˚˶)\n",
      "(・∀・)\n",
      "？????????？\n",
      "？？(\n",
      "…；\n",
      "？———\n",
      "…？\n",
      "。/´¯/)\n",
      "(ಡωಡ)\n",
      "？눈_눈\n",
      "……？）\n",
      "（￣▽￣）\n",
      "）。\n",
      "•̀㉨•́\n",
      "！！【\n",
      "∠(ᐛ」∠)＿\n",
      "⊙)！\n",
      "。？？？\n",
      "(˚☐˚!)/\n",
      "）！\n",
      "´∀`\n",
      "(⁎⁍̴̛ᴗ⁍̴̛⁎)\n",
      "ｌｏｌ\n",
      "？￣￣)σ\n",
      "┐（─__─）┌\n",
      "(´ﾟωﾟ)？\n",
      "このコメントは\n",
      "｡◕‿◕｡\n",
      "(=_=)。\n",
      "（✓\n",
      "？？＝＝\n",
      "°`)╮\n",
      "(=^▽^=)\n",
      "(??ω??)\n",
      "？!\n",
      "(*￣\n",
      "Σ(っ°Д°;)っ\n",
      "(；\n",
      "――\n",
      "（×）\n",
      "*^\n",
      "**\n",
      "(｡>∀<｡)\n",
      "。(/ω＼))\n",
      "✌️\n",
      "????????。\n",
      "：》\n",
      "(⑉°з°)♡\n",
      "！！！！！\n",
      "！！（\n",
      "〜(￣△￣〜)(〜￣△￣)〜\n",
      "?!\n",
      "><\n",
      "～（\n",
      "。。)\n",
      "··\n",
      "］。\n",
      "=_=\n",
      "(￣_￣)\n",
      "`)و\n",
      "（**\n",
      "←、←\n",
      "」∠)_\n",
      "?）……\n",
      "？[\n",
      "(∩_∩)\n",
      "??\n",
      "(°ー°〃)\n",
      "ºΔº\n",
      "）～(￣▽￣～)~\n",
      "**]\n",
      "？？？（\n",
      "⊙)\n",
      "(･_･)ﾉ⌒●~\n",
      "(｀・ω・´)\n",
      "(ÒωÓױ)\n",
      "⛵????\n",
      "_(:з)∠)_\n",
      "(?_?)\n",
      "):\n",
      "????????????\n",
      "·_·\n",
      "とけんかしています。\n",
      "。【\n",
      "。＝。＝\n",
      "？*～*\n",
      "。ε=(´ο｀*)))\n",
      "・ˍ・)\n",
      "!！！！\n",
      "(；￣д￣)\n",
      ")；\n",
      "》。\n",
      "；②\n",
      "(*￣ー￣)\n",
      "(・Д・)ノ\n",
      "\\。\n",
      "(᷅_᷄)\n",
      "_(：Ι」∠)_\n",
      "(´▽`)ﾉ\n",
      "？？？！！\n",
      "(≖͞_≖̥)\n",
      "(๑••๑)\n",
      "?）\n",
      "٩(๑`^´๑)۶)`Д´)\n",
      "ｕｐ\n",
      "٩(๑^\n",
      "》？\n",
      "@_@\n",
      "！????\n",
      "（）\n",
      "~。。。\n",
      "(´ー`)\n",
      "。——\n",
      "（?）\n",
      "。】【\n",
      "（’’*)\n",
      "’。\n",
      "（√）\n",
      "∠(ᐛ」∠)_\n",
      "(ー`´ー)\n",
      "。《\n",
      "（｀Δ´）ゞ\n",
      "(͡°͜ʖ͡°)✧\n",
      "´)\n",
      "(ι_)\n",
      "…????????????\n",
      "?(⊙`?′⊙)?\n",
      "=**\n",
      "。:)\n",
      "(#￣▽￣)==\n",
      "）、\n",
      "^ヽ)\n",
      "？！？！\n",
      "？）（\n",
      "(ーー゛)\n",
      "(||๐_๐)\n",
      "）~\n",
      "????。\n",
      "(●°\n",
      "(???????)\n",
      "！——\n",
      "！//\n",
      "。[\n",
      "？？？？？\n",
      "￣￣)σ\n",
      "ヾ(•ω•`)\n",
      "′|┛\n",
      "(｡･∀･)ﾉﾞ\n",
      "⊙∀⊙！\n",
      "’？\n",
      "」「\n",
      "^)/\n",
      "????????????????????????（\n",
      "(๑•ี_เ•ี๑)\n",
      "]）\n",
      "？」\n",
      "】？？？\n",
      "≤）\n",
      "(´_ゝ｀)\n",
      "/#/\n",
      "。②\n",
      "ﾟ;\n",
      "(゜゜)つロ\n",
      "！」\n",
      "（*/∇＼*）\n",
      "＼(^▽^)／！\n",
      "?_??\n",
      "！？\n",
      "|ू•ૅω•́)ᵎᵎᵎ\n",
      "···\n",
      "？？？\n",
      "！(๑°⌓°๑)\n",
      ":)\n",
      "??ω??)???\n",
      ":(\n",
      "~(๑•̀ㅁ•́ฅ)\n",
      "========\n",
      "！！！！！！\n",
      "(*σ´∀`)σ\n",
      "（？\n",
      "//←\n",
      "ﾟДﾟ)ﾉ\n",
      "（￣⊿￣）\n",
      "……】。\n",
      "_(:з」∠)_\n",
      "！)\n",
      "_(•̀ω•́」∠)_\n",
      "（*\n",
      "(ー_ー)!!\n",
      "？）。\n",
      "(。??︿??。)\n",
      "Õ_Õ\n",
      "、、\n",
      "╯﹏╰\n",
      "?」\n",
      "(?>?<?）\n",
      "(͔▪̆ω▪̆)͕\n",
      "…：…/\n",
      "!?\n",
      "******、\n",
      "(›´ω`‹)\n",
      "(ﾉД`)\n",
      "？(??ω??)\n",
      "）。。。\n",
      "。と\n",
      "눈_눈\n",
      "¬_¬｀\n",
      "(～￣▽￣)～\n",
      "！∞\n",
      "●ｖ●\n",
      "┐(‘～`；)┌\n",
      "ε=ε=(\n",
      "。(^_^)/~~\n",
      "(′～`；）\n",
      ":？？？\n",
      "◎)／！\n",
      "：【\n",
      "(ノ°ο°)ノ（\n",
      "ヽ(`Д´)\n",
      "(￣∀￣)\n",
      "(๑￣̫￣๑)\n",
      "：）\n",
      "|ω・）\n",
      "！(￣ε(#￣)\n",
      "┓(\n",
      "!!\n",
      "＃)\n",
      "(￣^￣゜)\n",
      "(⊙_⊙)\n",
      "(〜￣▽￣)〜（\n",
      "（｀Δ′）！\n",
      "•﹏•\n",
      "+（\n",
      "(〜￣▽￣)〜\n",
      "。。。。\n",
      "_(:з」∠)_。\n",
      "ヽ(￣д￣;)ノ\n",
      "⊙∀⊙……\n",
      "()\n",
      "（#\n",
      "(●—●)\n",
      "！？？？\n",
      "╭(°\n",
      "~(｀･ω･´)ﾉ\n",
      "!(‘ωก̀)\n",
      "(･∀･)/】\n",
      "╮(╯▽╰)╭(\n",
      "？•́ω•̀)¿¿¿\n",
      "????️？\n",
      "*（\n",
      "（？__？）\n",
      "✧٩(ˊωˋ*)و✧\n",
      "ǒ￣)\n",
      "!）\n",
      "(ಥ_ಥ)\n",
      "(#｀\n",
      "＠(￣￣)＠\n",
      "……】\n",
      "(#ﾟДﾟ)\n",
      "！????❤️\n",
      "……（\n",
      "(ﾟ\n",
      "？？？？？？？\n",
      "？！\n",
      "⊙∀⊙？\n",
      "(′ω`)。\n",
      "(。・`ω´・)\n",
      "ಥ_ಥ\n",
      "????？\n",
      "┐(´｀)┌\n",
      "(•́ὤ•̀)\n",
      "****\n",
      "？)\n",
      "】！\n",
      "(´ω`)\n",
      "？╭(╯ε╰)╮\n",
      "）】\n",
      "{:\n",
      "√。\n",
      "】、【\n",
      "￣^￣゜\n",
      "。。。。。\n",
      "@**\n",
      "(￢_￢)）\n",
      "(´ωก`)\n",
      "(・●・)\n",
      "~~！\n",
      "(。・`ω′·)\n",
      "⚠️\n",
      "・_・?\n",
      "==（\n",
      "ヽ（・＿・；)ノ\n",
      "☝(•̀˓◞•́)\n",
      "*♂*\n",
      "）；\n",
      "！！？\n",
      "）(#ﾟДﾟ)\n",
      "^*)）\n",
      "？～(￣▽￣～)~\n",
      "？？？？？？？？\n",
      "(゜ロ゜)\n",
      "***\n",
      "（｀Δ´）！\n",
      "°`)╮。\n",
      "]？\n",
      "×××\n",
      "！╮(￣▽￣)╭\n",
      "ớ₃ờ\n",
      "！）\n",
      "？⊙_⊙\n",
      "？？（\n",
      "ヽ(ー_ー)ノ\n",
      "(´ι_｀)\n",
      "（^_^）\n",
      "(=?ω?=)\n",
      "……(つд⊂)\n",
      "อิ_อิ\n",
      "……【\n",
      "(´﹏`；)\n",
      "(｀Д´*)\n",
      "~!!\n",
      "(╬•᷅д•᷄╬)\n",
      "）｀\n",
      "❌？？\n",
      "‘（\n",
      "·····\n",
      "……∠(ᐛ」∠)＿\n",
      "。(◔̮◔)\n",
      "^๑)۶\n",
      "Ծ‸Ծ\n",
      "うべきではない。\n",
      "(ﾉ__)ﾉ\n",
      "＊＊\n",
      "(≧\n",
      "（￣▽￣）～■□～（￣▽￣）\n",
      "(｀⌒´メ)\n",
      "눈_눈（\n",
      "——\n",
      "(;¬_¬)\n",
      "__\n",
      "************\n",
      "（>_<）！\n",
      "⊙﹏⊙\n",
      "(●◡●)\n",
      "???\n",
      "Σ(°△°|||)︴\n",
      "๑•́₃•̀๑\n",
      "。【//】\n",
      "。????\n",
      "？？？？\n",
      "(˘•ω•˘)\n",
      "(￣ヘ￣\n",
      "～～\n",
      "(~_~;)\n",
      "ଘ(੭ˊ꒳​ˋ)੭✧\n",
      "(ㅍ_ㅍ)\n",
      "(;｀\n",
      "Σ(ﾟдﾟ；)╰（‵□′）╯\n",
      "》：\n",
      "：‘\n",
      "：（\n",
      "ﾟ)\n",
      "˙)ว）\n",
      "。’\n",
      "』？\n",
      "!(⋟﹏⋞)\n",
      "(╯□╰)\n",
      "。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "?_?\n",
      "٩(´\n",
      "(?>?<?）)\n",
      "!!!\n",
      "りんね\n",
      "╮（﹀＿﹀）╭\n",
      "（｀Δ′）！**\n",
      "！！！（\n",
      "！:\n",
      "(･｀ω´･☝)\n",
      "～～～\n",
      "？*٩(๑´∀`๑)ง*\n",
      "ψ(｀?′)ψ\n",
      "(⁄⁄•⁄ω⁄•⁄⁄)\n",
      "(｡･ω･｡)\n",
      "°●)??」\n",
      "（\\\n",
      "(ღ˘⌣˘ღ)\n",
      "。。（\n",
      "(ノ｀⊿´)ノ\n",
      "）**（\n",
      "************——\n",
      "：①\n",
      "_(:ᗤ」ㄥ)_\n",
      "╮（╯＿╰）╭\n",
      "））。\n",
      "？。。。。。\n",
      "(ﾟДﾟ)\n",
      "(○ﾟεﾟ○)\n",
      "☁️\n",
      "’‘\n",
      "●^●\n",
      ")و\n",
      "╭(╯^╰)╮\n",
      "(´◔‸◔`)\n",
      "(๑‾ꇴ‾๑)\n",
      "？(\n",
      "ԅ(¯ㅂ¯ԅ)\n",
      "だ？\n",
      "へようこそ！\n",
      ")。\n",
      "(♯｀∧´)\n",
      "✧٩(ˊωˋ*)و\n",
      "ヾ(❀╹◡╹)ﾉ~\n",
      "？_(:з」∠)_\n",
      "(｡･ω･)\n",
      "_||\n",
      "(๑•́₃•̀๑)(\n",
      "（…）\n",
      "。。。(\n",
      "(>^ω^<)\n",
      "。/\n",
      "？？？（•́ω•̀)¿¿¿\n",
      "。】\n",
      "〣(ºΔº)〣\n",
      "にこんなことを\n",
      "╮(❛ᴗ❛)╭\n",
      "_(:зゝ∠)_\n",
      "(ﾉ｀⊿´)ﾉ\n",
      "╮(╯_╰)╭\n",
      "。→_→\n",
      "）？\n",
      "。（▿）\n",
      "(¯﹃¯)\n",
      "(*╹▽╹*)\n",
      "]。\n",
      "(《\n",
      "๑_๑\n",
      "！！！\n",
      "(*^\n",
      "(……\n",
      "。)\n",
      "））\n",
      "((*゜Д゜)ゞ\n",
      "。(´•ω•)ノ(_`)\n",
      "。╮(•́ω•̀)╭\n",
      "～\\(￣▽￣)/\n",
      "(￣ε(#￣)☆╰╮\n",
      "…。\n",
      "(~￣△￣)~\n",
      "?。\n",
      "(*｀▽´*)\n",
      "╮(╯▽╰)╭\n",
      "』≠『\n",
      "。）。\n",
      "！（\n",
      "？？！！！\n",
      "）：\n",
      "（▼へ▼メ）\n",
      "}{\n",
      "(ŐдŐ๑)\n",
      "(￣▽￣)\n",
      "ε(•́ω•̀๑)\n",
      "(^_^)\n",
      "。″\n",
      "(｢･ω･)｢\n",
      "(￣￣)\n",
      "(ﾟДﾟ)ﾉ\n",
      "】。\n",
      "。。。。。。。。。。。。。。。。\n",
      "！！\n",
      "_(:::з」∠)_\n",
      "_(:\n",
      "】）\n",
      "٩(ᐛ)و\n",
      "(๑╹∀╹๑)\n",
      "(*・ω・)✄╰ひ╯\n",
      "୧(๑•̀ㅁ•́๑)૭✧\n",
      "（＃－－）\n",
      "==、\n",
      "ヽ(•̀ω•́)ゝ\n",
      "？????️\n",
      "？(・◇・)\n",
      "ԅ(¯ㅂ¯ԅ)。\n",
      "==\n",
      "』『\n",
      "。=￣ω￣=（\n",
      "(>﹏<)\n",
      "￣)\n",
      "=。=\n",
      "|˛˙꒳​˙)♡\n",
      "(*/ω＼*)\n",
      "????????\n",
      "。_(:з)∠)_\n",
      "(=・ω・=)\n",
      "〒_〒）\n",
      "。。）\n",
      "。ヽ(ー_ー)ノ\n",
      "！……\n",
      "こんな\n",
      "(＾Ｕ＾)ノ\n",
      "？？\n",
      "（@\n",
      "？***\n",
      "？*****\n",
      "!(\n",
      "（↑\n",
      "？@_@）\n",
      "(´◉◉)\n",
      "。【///】\n",
      "……?\n",
      "゜▽゜)\n",
      "(・。・)\n",
      "）。。\n",
      "(#^^#)？\n",
      "……——\n",
      "π_π\n",
      "(´∵｀)\n",
      "××\n",
      "******……\n",
      "￣へ￣\n",
      "´・◡・｀)\n",
      "(｡•́︿•̀｡)\n",
      "╮(￣▽￣)╭\n",
      "？？！\n",
      "(´▽`ʃƪ)\n",
      "……？\n",
      "？？？？？？？？？？\n",
      "(•̥́ˍ•̀ू)\n",
      "(๑❛ꆚ❛๑)\n",
      "？？？？、\n",
      ")┏\n",
      "、、、\n",
      "→_→\n",
      "(≖_≖)\n",
      "******。\n",
      "###\n",
      "。。。（\n",
      "????\n",
      "！@\n",
      "がはっきりしています。\n",
      "？(´∵｀)\n",
      "￥￥\n",
      "。。(\n",
      "┑(￣Д￣)┍\n",
      "⊙▽⊙\n",
      "？…\n",
      "=/=\n",
      "(:з」∠)_\n",
      "……????\n",
      "(*´◐∀◐`*)\n",
      "。゜(ノ)´Д(ヾ)゜。゜\n",
      "ԅ(¯㉨¯ԅ)\n",
      "°●)​\n",
      "〣(\n",
      "╮(‵▽′)╭\n",
      "(ｷ｀ﾟДﾟ´)!!\n",
      "_:(´_`」∠):_\n",
      "¯\\_(ツ)_/¯\n",
      "。③\n",
      "))\n",
      "~)\n",
      "|ω･`)\n",
      "……）\n",
      "｢(ﾟﾍﾟ)\n",
      "(҂⌣̀_⌣́)\n",
      ">_<\n",
      "……๏̯͡´_>`\n",
      "。<(__)>\n",
      "~~~~~\n",
      "？？）\n",
      "(*?ω?)?╰ひ╯\n",
      "(˙ω˙)\n",
      "。。。\n",
      "(。`ω´)\n",
      "????（\n",
      "(ﾉω･｀\n",
      "┗|｀\n",
      "めない\n",
      "；③\n",
      "^^\n",
      "）+\n",
      "？。。。\n",
      "……\n",
      "ヽ(✿ﾟ▽ﾟ)ノ\n",
      "(๑ó﹏ò๑)。\n",
      "ヘ(￣ω￣ヘ)♪\n",
      "？）(´◉◉)\n",
      "』。\n",
      "、*\n",
      "？）……\n",
      "（？）\n",
      "！！！！\n",
      "。^^\n",
      "。（\n",
      "(´▽`)\n",
      "ㄟ(▔\n",
      "）)\n",
      "￣///)}\n",
      "）（\n",
      "_///\n",
      "(？)\n",
      ")∠)_\n",
      "(๑ʘ̅дʘ̅๑)!!!\n",
      "？……\n",
      "？！！！\n",
      "|･ω･｀)\n",
      "ԅ(¯﹃¯ԅ)\n",
      "(?Д?)?\n",
      "？？＋\n",
      "ヾ^_^♪\n",
      "ヽ(ﾟ∀ﾟ)ﾉ\n",
      "￣///)\n",
      "(⊙﹏⊙)\n",
      "。。。(✘_✘)\n",
      "?(￣△￣?)\n",
      "】【\n",
      "?·?\n",
      "٩(\n",
      "(๑•̀ω•́๑)\n",
      "ヽ(ﾟ∀ﾟ)ﾉ!\n",
      "、。\n",
      "！◠‿◠\n",
      "。(≖_≖)\n",
      "；）\n",
      "？【\n",
      "》、《\n",
      "？（\n",
      "。、\n",
      "○○\n",
      "•́ω•̀)¿¿¿\n",
      "。。？\n",
      "…|_・)\n",
      "][\n",
      "(ｰ̀дｰ́)\n",
      "⑧？\n",
      "(๑•̀ㅂ•́)و✧\n",
      "あびこ(\n",
      "]∶\n",
      "！→_→\n",
      "。+\n",
      ":【\n",
      "！(ノ=Д=)ノ┻━┻\n",
      "******\n",
      "`)و`)و\n",
      "？(ಡωಡ)\n",
      "(●´∇｀●)\n",
      "(￣\n",
      "‎´•ﻌ•`\n",
      "´_>`？？？\n",
      "(?˙ー˙?)\n",
      "﻿⊙∀⊙！\n",
      "？）、\n",
      "(˶‾᷄⁻̫‾᷅˵)\n",
      "_(:з)∠)_？\n",
      "(•́ὤ•̀）\n",
      "？！！\n",
      "=͟͟͞͞ʕ•̫͡•ʔ\n",
      ">）\n",
      "(╯‵□′)╯︵┻━┻\n",
      "(⊙\n",
      "_(:Ⅰ」∠)_\n",
      "》？？？\n",
      "。*\n",
      "（｡ò∀ó｡）\n",
      ")？\n",
      "(=_=)\n",
      "(￣へ￣)\n",
      "(๑‾᷅^‾᷅๑)\n",
      "。。。。。。。。。\n",
      "•̀ᴗ)✧\n",
      "？???\n",
      "（←\n",
      "ヽ(‘⌒´メ)ノ\n",
      "！(ι_)\n",
      "】_(:зゝ∠)_\n",
      "（✓）\n",
      "（）。\n",
      "？。。\n",
      "？(；￣д￣)\n",
      "(๑ó﹏ò๑)\n",
      "。。\n",
      "。。。）\n",
      "∪･ω･∪\n",
      ");\n",
      "？？？(\n",
      "~！\n",
      "…）\n",
      "～(￣▽￣～)\n",
      "！！！！！！！！\n",
      "´_>`\n",
      "？。\n",
      "`ω)✧\n",
      "~_~\n",
      "*)\n",
      "？！？\n",
      "－－\n",
      "（・ω・)=つ≡つ\n",
      "？_||\n",
      "](｡•́ωก̀｡)｡\n",
      "~\\(≧▽≦)/~\n",
      "………\n",
      "。///\n",
      "）┐（─__─）┌\n",
      "？(●—●)\n",
      "(^_^)／\n",
      "ｰ̀εｰ́\n",
      "。_||\n",
      "～(´ー｀～)\n",
      "？]\n",
      "(￢_￢)\n",
      "？】\n",
      "!!!!\n",
      "!(๑•̀ㅂ•́)و✧\n",
      "*。\n",
      "~）\n",
      "：*********\n",
      "(＊｀д´)\n",
      "ψ(⃔๑⃙⃘ω๑⃙⃘)⃕↝♡︎ʾʾ\n",
      "？！(\n",
      "…(#ﾟДﾟ)\n",
      "||\n",
      "？——\n",
      "????）\n",
      "？？。。。\n",
      "_#\n",
      "_(:_」∠)_\n",
      "╭(╯ε╰)╮\n",
      "。（’’*)\n",
      "。？\n",
      "(๑¯ω¯๑)\n",
      "↑↑\n",
      "╮(??ω??)╭\n",
      "⊙ω⊙\n",
      "╮(•́ω•̀)╭\n",
      "（;≥\n",
      "(｡ì_í｡)\n",
      "☀????☀\n",
      "(๑˙ー˙๑)\n",
      "←_←←_←\n",
      "いつも\n",
      "~！！\n",
      "》《\n",
      "）/\n",
      "*********\n",
      "=￣ω￣=\n",
      "(づ●─●)づ\n",
      ";);)\n",
      "(*\n",
      "？:）\n",
      "(*´_ゝ｀)\n",
      "/]\n",
      "(•̀ω⁃᷄)✧\n",
      "⊙∀⊙\n",
      "(๑•ั็ω•็ั๑)\n",
      "）？？\n",
      "(⊙_⊙?)\n",
      "（︶︿︶）！\n",
      "::>_<::）\n",
      "ヽ(;^\n",
      "←_←\n",
      "(^･ｪ･^)\n",
      "）……\n",
      ":？？？？\n",
      "？＼(◎\n",
      "？????\n",
      "***、\n",
      "。）\n",
      "…………\n",
      "←←\n",
      "（\\#_)\\┯━┯\n",
      "(눈_눈)\n",
      "ԅ(✧_✧ԅ)\n",
      "(≖‿≖)\n",
      "╰（‵□′）╯\n",
      "|ω•`)\n",
      "。（）\n",
      "」。\n",
      "！。\n",
      "。。。@#$&#$@*&%$<。。。\n",
      "_。\n",
      "（▿）\n",
      "。(\n",
      "\\(^\n",
      "。。。。。。\n",
      "！【\n",
      "…………………………\n",
      "＝？\n",
      "Σ(ﾟ∀ﾟﾉ)ﾉ\n",
      "———\n",
      "。๛ก(ｰ̀ωｰ́ก)\n",
      "(〃′\n",
      "//\n",
      "(◔◡◔)\n",
      "？~\n",
      "ε=(´ο｀*)))\n",
      ")（╯－＿－）╯╧╧\n",
      "<。\n",
      "：？\n",
      ")〣？\n",
      "～(￣▽￣～)~\n",
      "<(__)>\n",
      "≦)\n",
      "？(^_^)\n",
      "(๑>؂<๑）\n",
      "？！_(:зゝ∠)_\n",
      "=￣ω￣=（\n",
      "。。(•̥́ˍ•̀ू)\n",
      "。。。。。。。。\n",
      "~（\n",
      "(^ω^)\n",
      "=、=\n",
      "://\n",
      "(๑ò︵ò๑)\n",
      "(ฅ>ω<*ฅ)\n",
      "~~\n",
      "（╯‵□′）╯︵┴─┴\n",
      "？？？……\n",
      "ರ_ರ\n",
      "~~~\n",
      "（(ง\n",
      "。。。。。。。。。。。。。。\n",
      "！(\n",
      "？）\n",
      "==。\n",
      "》)\n",
      "。。。？\n",
      "(?｀?ω?)?\n",
      "(´･_･`)\n",
      "(´；ω；`)\n",
      "？→_→\n",
      "(°ー°〃)）\n",
      "<(｀^´)>\n",
      "。。？？\n",
      "ಠ_ಠ\n",
      "`)\n",
      "。|･ω･｀)\n",
      "\n",
      "1458\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "# jieba.load_userdict(\"../dataset/1000000-dict.txt\")\n",
    "\n",
    "\"去除指定无用的符号\"\n",
    "puncts = ['，', '“','”']\n",
    "res = []\n",
    "def clean_text(x):\n",
    "    if str(x) == 'nan':\n",
    "        return ''\n",
    "    x = x.strip()\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct,'')\n",
    "    return x\n",
    "\n",
    "def count_spec(x):\n",
    "    if str(x) != 'nan':\n",
    "        q = x.split()\n",
    "        if len(q) > 0:\n",
    "            for item in q:\n",
    "                if len(item) > 1:\n",
    "                    res.append(item)\n",
    "\n",
    "\"让文本只保留汉字、英文和数字\"\n",
    "def is_chinese_alphabet_number(xchar):\n",
    "    if (xchar >= u'\\u4e00' and xchar <= u'\\u9fa5') or (xchar >= u'\\u0041' and xchar <= u'\\u005a') or (xchar >= u'\\u0061' and xchar <= u'\\u007a'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\"将汉字、英文和数字保留下来\"\n",
    "def keep_text(x):\n",
    "    out_str=''\n",
    "    for i in x:\n",
    "        if not is_chinese_alphabet_number(i):\n",
    "            out_str = out_str+i\n",
    "        else:\n",
    "            out_str = out_str+' '\n",
    "    return out_str\n",
    "\n",
    "def seg_sentence(sentence,stopwords):\n",
    "    \"对句子进行分词和去除停用词\"\n",
    "    jieba.load_userdict(\"../dataset/user_dict.txt\")\n",
    "    sentence_lower = sentence.lower()\n",
    "    sentence_seged = jieba.cut(sentence_lower, cut_all=False)\n",
    "    outstr=''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "                outstr+=word\n",
    "                outstr+=\" \"\n",
    "    return outstr\n",
    "\n",
    "def build_vocab(sentences,verbose=True):\n",
    "    \"追踪训练词汇表，遍历所有文本对单词进行计数\"\n",
    "    vocab={}\n",
    "    for sentence in tqdm(sentences,disable=(not verbose)):\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# def texts_to_sequences(sentences,vocab,verbose=True):\n",
    "#     seq_sentences=[]\n",
    "#     unk_vec=np.random.random(embed_size)*0.5\n",
    "#     unk_vec=unk_vec-unk_vec.mean()\n",
    "#     for sentence in tqdm(sentences,disable=(not verbose)):\n",
    "#         seq_sentence=[]\n",
    "#         for word in sentence.split():\n",
    "#             seq_sentence.append(vocab.get(word,unk_vec))\n",
    "#         seq_sentences.append(seq_sentence)\n",
    "#     return seq_sentences\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vocab_size = 100\n",
    "    dataset = pd.read_csv('pre_dataset.csv')\n",
    "    #创建自定义词表\n",
    "    dataset[\"content\"]=dataset[\"content\"].apply(lambda x: count_spec(x))\n",
    "#     dataset[\"content\"]=dataset[\"content\"].apply(lambda x: keep_text(x))\n",
    "    #dataset[\"content\"]=dataset[\"content\"].apply(lambda x: seg_sentence(x, stopwords))\n",
    "#     dataset.to_csv('pre_dataset_1.csv', index = False)\n",
    "    set_res = set(res)\n",
    "    arr = list(set_res)\n",
    "    number = []\n",
    "    symbol = []\n",
    "    \n",
    "    for item in arr:\n",
    "        flag = 1\n",
    "        for xchar in item:\n",
    "            if xchar >= u'\\u0030' and xchar <= u'\\u0039':\n",
    "                number.append(item)\n",
    "                flag = 0\n",
    "                break\n",
    "        if flag == 1:\n",
    "            symbol.append(item)\n",
    "            \n",
    "    number1 = ''\n",
    "    symbol1 = ''\n",
    "    for item in number:\n",
    "        item += '\\n'\n",
    "        number1 += item\n",
    "    for item in symbol:\n",
    "        item += '\\n'\n",
    "        symbol1+= item\n",
    "    print(number1)\n",
    "    print(symbol1)\n",
    "    f1 = open('number.txt','a',encoding = 'utf-8')\n",
    "    f1.write(number1)\n",
    "    f1.close()\n",
    "    f2 = open('symbol.txt','a',encoding = 'utf-8')\n",
    "    f2.write(symbol1)\n",
    "    f2.close()\n",
    "    print(len(set_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-0769daeaf22d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Embedding, Input, Dense, Conv2D, MaxPooling2D, Concatenate, Reshape, Dropout, Flatten, Lambda, LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "import re\n",
    "import codecs\n",
    "from keras import initializers, regularizers, constraints\n",
    "def clean_text(x):\n",
    "    x = x.strip()\n",
    "    for punct in PUCNTS:\n",
    "        x = x.replace(punct,'')\n",
    "    return x\n",
    "\n",
    "\"让文本只保留汉字、英文和数字\"\n",
    "def is_chinese_alphabet_number(xchar):\n",
    "    if (xchar >= u'\\u4e00' and xchar <= u'\\u9fa5') or (xchar >= u'\\u0030' and xchar <= u'\\u0039') or (xchar >= u'\\u0041' and xchar <= u'\\u005a') or (xchar >= u'\\u0061' and xchar <= u'\\u007a'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\"将汉字、英文和数字保留下来\"\n",
    "def keep_text(x):\n",
    "    out_str=''\n",
    "    for i in x:\n",
    "        if is_chinese_alphabet_number(i):\n",
    "            out_str = out_str+i\n",
    "    return out_str\n",
    "\n",
    "def seg_sentence(sentence,stopwords):\n",
    "    \"对句子进行分词和去除停用词\"\n",
    "    jieba.load_userdict(\"dataset/user_dict.txt\")\n",
    "    sentence_lower = sentence.lower()\n",
    "    sentence_seged = jieba.cut(sentence_lower, cut_all=False)\n",
    "    outstr=''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "                outstr+=word\n",
    "                outstr+=\" \"\n",
    "    return outstr\n",
    "\n",
    "def build_vocab(sentences,verbose=True):\n",
    "    \"追踪训练词汇表，遍历所有文本对单词进行计数\"\n",
    "    vocab={}\n",
    "    for sentence in tqdm(sentences,disable=(not verbose)):\n",
    "        #print(sentence)\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def texts_to_sequences(sentences,vocab,verbose=True):\n",
    "    seq_sentences=[]\n",
    "    #pdb.set_trace()\n",
    "    unk_vec=np.random.random(embed_size)*0.5\n",
    "    unk_vec=unk_vec-unk_vec.mean()\n",
    "    for sentence in tqdm(sentences,disable=(not verbose)):\n",
    "        seq_sentence=[]\n",
    "        for word in sentence.split():\n",
    "            seq_sentence.append(vocab.get(word,unk_vec))\n",
    "        seq_sentences.append(seq_sentence)\n",
    "    return seq_sentences\n",
    "\n",
    "def load_and_prec():\n",
    "    vocab_size = 100\n",
    "    count = 0\n",
    "    if not os.path.exists('dataset/pre_dataset.csv') :\n",
    "        #文件读取\n",
    "        dataset = pd.read_csv('dataset/model_dataset.csv')\n",
    "        #创建停用词列表\n",
    "        stopwords = ['的', '呀', '这', '那', '就', '的话', '如果', '了', '建议', '说','是', '吧','我','你','在','这个','被','啊','和','吗','觉得','就是','应该','认为','很','感觉','可能','会','下面']\n",
    "        #创建自定义词表\n",
    "        dataset[\"content\"]=dataset[\"content\"].apply(lambda x: clean_text(x))\n",
    "        dataset[\"content\"]=dataset[\"content\"].apply(lambda x: keep_text(x))\n",
    "        dataset[\"content\"]=dataset[\"content\"].apply(lambda x: seg_sentence(x, stopwords))\n",
    "        dataset.to_csv('dataset/pre_dataset.csv', index = False)\n",
    "    else:\n",
    "        dataset = pd.read_csv('dataset/pre_dataset.csv')\n",
    "        for item in dataset['content']:\n",
    "            if str(item) == 'nan':\n",
    "                dataset['content'][count] = ''\n",
    "            count += 1\n",
    "\n",
    "    labelMap = {1: 0, 2: 1, 4: 2}\n",
    "    labels = dataset['vote'].map(labelMap)\n",
    "    texts = dataset['content'].values\n",
    "    labels = labels.values\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index)) #Found 39468 unique tokens.词频词典\n",
    "    \n",
    "    word_num = [len(text) for text in sequences]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    _ = plt.hist(word_num, bins = 100)\n",
    "    plt.xlabel('word number')\n",
    "    plt.ylabel('Freq')\n",
    "    plt.show()\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_LEN, padding = 'post') \n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape) #(120000, 40)\n",
    "    print('Shape of label tensor:', labels.shape) #(120000, 3)\n",
    "    \n",
    "    x_train_val, x_test, y_train_val, y_test = train_test_split(data, labels, test_size = VAL_SPLIT, random_state = 2021)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size = VAL_SPLIT, random_state = 2021)\n",
    "    \n",
    "    print(\"Train shape: \",x_train.shape)   # (97200, 40)\n",
    "    print(\"Val shape: \",x_val.shape)   # (10800, 40)\n",
    "    print(\"Test shape: \",x_test.shape) # (12000, 40)\n",
    "    \n",
    "    np.random.seed(2021)\n",
    "    trn_idx=np.random.permutation(len(x_train))\n",
    "    val_idx=np.random.permutation(len(x_val))\n",
    "    print(x_train[0])\n",
    "    x_train=x_train[trn_idx]\n",
    "    x_val=x_val[val_idx]\n",
    "    y_train=y_train[trn_idx]\n",
    "    y_val=y_val[val_idx]\n",
    "    print(x_train[0])\n",
    "    return dataset, x_train, x_val, x_test, y_train, y_val, y_test, tokenizer\n",
    "\n",
    "def input_embed():\n",
    "    embeddings_index = {}\n",
    "    with open('dataset/1000000-small.txt','r') as f:\n",
    "        for i,line in tqdm(enumerate(f),disable=(not verbose)):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            codes = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = codes\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def embed_matrix(word_index, embeddings_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "dataset, x_train, x_val, x_test, y_train, y_val, y_test, tokenizer = load_and_prec()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
